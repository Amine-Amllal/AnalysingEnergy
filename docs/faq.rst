Questions Fr√©quentes (FAQ)
==========================

Cette section r√©pond aux questions les plus fr√©quemment pos√©es concernant le projet AnalysingEnergy.

Questions g√©n√©rales
-------------------

Qu'est-ce que le projet AnalysingEnergy ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Le projet AnalysingEnergy est un syst√®me complet d'analyse et de pr√©diction de la production d'√©nergie verte utilisant des r√©seaux de neurones LSTM (Long Short-Term Memory). Il analyse le dataset BanE-16 pour pr√©dire la g√©n√©ration maximale d'√©nergie en fonction de variables m√©t√©orologiques et de demande √©nerg√©tique.

**Caract√©ristiques principales** :

- Pr√©diction de production d'√©nergie avec IA
- Interface Streamlit interactive
- Optimisation d'hyperparam√®tres avec Optuna  
- Documentation compl√®te et exemples pratiques

Quelle est la pr√©cision du mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Le mod√®le LSTM optimis√© atteint une pr√©cision remarquable :

- **RMSE** : ~291 MW (erreur quadratique moyenne)
- **MAE** : ~195 MW (erreur absolue moyenne)
- **MAPE** : ~3.2% (erreur pourcentage moyenne)
- **R¬≤** : ~0.91 (coefficient de d√©termination)

Ces performances permettent une planification √©nerg√©tique fiable.

Puis-je utiliser le projet avec mes propres donn√©es ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Oui, absolument ! Le projet est con√ßu pour √™tre adaptable :

**Format requis** :

.. code-block:: python

    # Colonnes minimales requises
    colonnes_requises = [
        'temp2_max(c)', 'temp2_min(c)', 'temp2_ave(c)',
        'suface_pressure(pa)', 
        'wind_speed50_max(m/s)', 'wind_speed50_min(m/s)', 'wind_speed50_ave(m/s)',
        'prectotcorr', 'total_demand(mw)',
        'max_generation(mw)'  # Variable cible
    ]

**√âtapes d'adaptation** :

1. Formatez vos donn√©es selon la structure attendue
2. Ajustez les chemins dans les notebooks
3. R√©entra√Ænez le mod√®le sur vos donn√©es
4. Validez les performances

**Conseils** :

- Assurez-vous d'avoir au moins 2-3 ans de donn√©es
- V√©rifiez la qualit√© et la coh√©rence temporelle
- Adaptez les plages de normalisation si n√©cessaire

Questions techniques
--------------------

Quelles sont les d√©pendances requises ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**D√©pendances principales** :

.. code-block:: bash

    # Machine Learning
    tensorflow>=2.10.0
    scikit-learn>=1.1.0
    optuna>=3.0.0
    
    # Data Science
    pandas>=1.5.0
    numpy>=1.21.0
    matplotlib>=3.5.0
    seaborn>=0.11.0
    
    # Interface
    streamlit>=1.20.0
    plotly>=5.10.0

**Installation** :

.. code-block:: bash

    pip install -r requirements.txt

**Configuration GPU** (optionnelle) :

.. code-block:: bash

    # Pour acc√©l√©ration GPU
    pip install tensorflow-gpu
    # V√©rifier CUDA/cuDNN compatibility

Comment optimiser les performances ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Optimisation d'entra√Ænement** :

.. code-block:: python

    # Utilisation de callbacks optimis√©s
    callbacks = [
        EarlyStopping(patience=10, restore_best_weights=True),
        ReduceLROnPlateau(factor=0.5, patience=5),
        ModelCheckpoint('best_model.h5', save_best_only=True)
    ]

**Optimisation GPU** :

.. code-block:: python

    # Configuration m√©moire GPU
    import tensorflow as tf
    
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        tf.config.experimental.set_memory_growth(gpus[0], True)

**Optimisation des donn√©es** :

- Utilisez des batch sizes appropri√©s (32-128)
- Impl√©mentez des g√©n√©rateurs pour gros datasets
- Optimisez la longueur des s√©quences (30-60 timesteps)

Pourquoi utiliser LSTM plut√¥t que d'autres mod√®les ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Avantages des LSTM pour l'√©nergie** :

1. **M√©moire temporelle** : Capture les d√©pendances long terme
2. **Gestion des s√©quences** : Id√©al pour les s√©ries temporelles
3. **Robustesse** : R√©sistant au gradient vanishing
4. **Flexibilit√©** : Adaptable √† diff√©rents horizons de pr√©diction

**Comparaison avec alternatives** :

.. list-table:: Comparaison des mod√®les
   :widths: 20 20 20 20 20
   :header-rows: 1

   * - Mod√®le
     - RMSE
     - Temps
     - Complexit√©
     - Interpr√©tabilit√©
   * - LSTM
     - 291 MW
     - Moyen
     - √âlev√©e
     - Faible
   * - Random Forest
     - 320 MW
     - Rapide
     - Moyenne
     - √âlev√©e
   * - Linear Reg.
     - 380 MW
     - Tr√®s rapide
     - Faible
     - Tr√®s √©lev√©e

Comment faire une pr√©diction simple ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Exemple minimal** :

.. code-block:: python

    import joblib
    from tensorflow.keras.models import load_model
    
    # Charger le mod√®le et scalers
    model = load_model('Notebooks/models/final_model 291.19.h5')
    scaler_X = joblib.load('Notebooks/scalers/X_train_scaler.pkl')
    scaler_y = joblib.load('Notebooks/scalers/y_train_scaler.pkl')
    
    # Pr√©parer les donn√©es
    new_data = [[25, 20, 22.5, 101000, 6, 3, 4.5, 0.1, 7000]]
    scaled_data = scaler_X.transform(new_data)
    input_data = scaled_data.reshape(1, 1, -1)
    
    # Pr√©diction
    prediction = model.predict(input_data)
    result = scaler_y.inverse_transform(prediction)[0, 0]
    
    print(f"Production pr√©dite: {result:.0f} MW")

Comment interpr√©ter les r√©sultats ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**M√©triques cl√©s** :

- **RMSE < 300 MW** : Excellente pr√©cision
- **MAPE < 5%** : Erreur acceptable pour la planification
- **R¬≤ > 0.85** : Mod√®le explique bien la variance

**Analyse des r√©sidus** :

.. code-block:: python

    # V√©rification de la qualit√©
    residuals = y_true - y_pred
    
    # Tests de normalit√©
    from scipy import stats
    statistic, p_value = stats.jarque_bera(residuals)
    
    if p_value > 0.05:
        print("‚úÖ R√©sidus normalement distribu√©s")
    else:
        print("‚ö†Ô∏è V√©rifier la sp√©cification du mod√®le")

**Intervalles de confiance** :

Utilisez la variance des r√©sidus pour estimer l'incertitude :

.. code-block:: python

    std_residuals = np.std(residuals)
    confidence_interval = 1.96 * std_residuals  # 95% CI
    
    print(f"Pr√©diction: {prediction:.0f} ¬± {confidence_interval:.0f} MW")

Comment entra√Æner un nouveau mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Processus complet** :

.. code-block:: python

    # 1. Chargement et pr√©paration
    from notebooks.data_preprocessing import DataPreprocessor
    
    preprocessor = DataPreprocessor(sequence_length=60)
    X, y = preprocessor.fit_transform(data)
    
    # 2. Construction du mod√®le
    from notebooks.lstm_models import build_optimized_lstm
    
    model = build_optimized_lstm(
        input_shape=(60, 9),
        lstm_units=[64, 32],
        dropout_rate=0.2
    )
    
    # 3. Entra√Ænement
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=100,
        batch_size=32,
        callbacks=callbacks
    )
    
    # 4. Sauvegarde
    model.save('mon_modele.h5')
    joblib.dump(preprocessor.scaler_X, 'scaler_X.pkl')
    joblib.dump(preprocessor.scaler_y, 'scaler_y.pkl')

Questions sur les donn√©es
--------------------------

Quelle est la qualit√© requise des donn√©es ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Crit√®res minimaux** :

1. **Compl√©tude** : < 5% de valeurs manquantes
2. **Coh√©rence temporelle** : Pas de gaps > 7 jours
3. **Plausibilit√©** : Valeurs dans les plages attendues
4. **R√©solution** : Donn√©es journali√®res minimum

**V√©rification qualit√©** :

.. code-block:: python

    def check_data_quality(data):
        issues = []
        
        # Valeurs manquantes
        missing_pct = data.isnull().sum() / len(data) * 100
        high_missing = missing_pct[missing_pct > 5]
        if len(high_missing) > 0:
            issues.append(f"Colonnes avec >5% manquant: {list(high_missing.index)}")
        
        # Valeurs aberrantes
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            Q1, Q3 = data[col].quantile([0.25, 0.75])
            IQR = Q3 - Q1
            outliers = data[(data[col] < Q1-1.5*IQR) | (data[col] > Q3+1.5*IQR)]
            outlier_pct = len(outliers) / len(data) * 100
            if outlier_pct > 10:
                issues.append(f"Colonne {col}: {outlier_pct:.1f}% d'outliers")
        
        return issues

Comment g√©rer les valeurs manquantes ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Strat√©gies recommand√©es** :

.. code-block:: python

    # 1. Interpolation temporelle
    data_interpolated = data.interpolate(method='time')
    
    # 2. Forward fill pour courtes s√©quences
    data_filled = data.fillna(method='ffill', limit=3)
    
    # 3. Moyenne mobile pour variables m√©t√©o
    data['temp2_ave(c)'].fillna(
        data['temp2_ave(c)'].rolling(7, center=True).mean(),
        inplace=True
    )
    
    # 4. R√©gression pour relations complexes
    from sklearn.linear_model import LinearRegression
    
    # Pr√©dire temp√©rature min √† partir de max et moyenne
    mask = data['temp2_min(c)'].isna()
    if mask.sum() > 0:
        features = ['temp2_max(c)', 'temp2_ave(c)']
        X_train = data[~mask][features]
        y_train = data[~mask]['temp2_min(c)']
        
        reg = LinearRegression().fit(X_train, y_train)
        data.loc[mask, 'temp2_min(c)'] = reg.predict(data[mask][features])

Puis-je utiliser des donn√©es horaires ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Oui, mais avec adaptations :

**Modifications n√©cessaires** :

.. code-block:: python

    # 1. Agr√©gation journali√®re
    def aggregate_hourly_to_daily(hourly_data):
        daily_data = hourly_data.resample('D').agg({
            'temperature': ['min', 'max', 'mean'],
            'wind_speed': ['min', 'max', 'mean'],
            'pressure': 'mean',
            'precipitation': 'sum',
            'generation': 'max',
            'demand': 'mean'
        })
        
        # Aplatir les colonnes multi-niveau
        daily_data.columns = ['_'.join(col).strip() for col in daily_data.columns]
        
        return daily_data
    
    # 2. Sequence length adaptation
    # Pour donn√©es horaires: sequence_length = 24*7 (une semaine)
    # Pour donn√©es journali√®res: sequence_length = 60 (2 mois)

Questions sur le d√©ploiement
-----------------------------

Comment d√©ployer l'application en production ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Option 1: Serveur local**

.. code-block:: bash

    # Configuration pour production
    streamlit run interface/app.py \
        --server.port 8501 \
        --server.address 0.0.0.0 \
        --server.headless true

**Option 2: Docker**

.. code-block:: dockerfile

    FROM python:3.9-slim
    
    WORKDIR /app
    COPY requirements.txt .
    RUN pip install -r requirements.txt
    
    COPY . .
    EXPOSE 8501
    
    CMD ["streamlit", "run", "interface/app.py", "--server.port=8501", "--server.address=0.0.0.0"]

**Option 3: Cloud (Streamlit Cloud)**

1. Push code to GitHub
2. Connect Streamlit Cloud to repository  
3. Deploy automatically

**S√©curisation** :

.. code-block:: python

    # config.toml
    [server]
    enableCORS = false
    enableXsrfProtection = true
    maxUploadSize = 200

Comment int√©grer via API REST ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Cr√©ation d'une API Flask** :

.. code-block:: python

    from flask import Flask, request, jsonify
    import joblib
    from tensorflow.keras.models import load_model
    
    app = Flask(__name__)
    
    # Charger les mod√®les au d√©marrage
    model = load_model('models/final_model.h5')
    scaler_X = joblib.load('scalers/X_scaler.pkl')
    scaler_y = joblib.load('scalers/y_scaler.pkl')
    
    @app.route('/predict', methods=['POST'])
    def predict():
        try:
            # R√©cup√©ration des donn√©es
            data = request.json
            
            # Validation des inputs
            required_fields = [
                'temp_max', 'temp_min', 'temp_avg',
                'pressure', 'wind_max', 'wind_min', 'wind_avg',
                'precipitation', 'demand'
            ]
            
            if not all(field in data for field in required_fields):
                return jsonify({'error': 'Missing required fields'}), 400
            
            # Pr√©paration
            input_data = [[data[field] for field in required_fields]]
            scaled_data = scaler_X.transform(input_data)
            reshaped_data = scaled_data.reshape(1, 1, -1)
            
            # Pr√©diction
            prediction = model.predict(reshaped_data, verbose=0)
            result = scaler_y.inverse_transform(prediction)[0, 0]
            
            return jsonify({
                'prediction_mw': float(result),
                'confidence': 'high' if abs(result - 7000) < 1000 else 'medium',
                'timestamp': datetime.now().isoformat()
            })
            
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=5000)

**Utilisation de l'API** :

.. code-block:: python

    import requests
    
    # Donn√©es d'exemple
    data = {
        'temp_max': 25.0,
        'temp_min': 18.0,
        'temp_avg': 21.5,
        'pressure': 101300,
        'wind_max': 8.0,
        'wind_min': 2.0,
        'wind_avg': 5.0,
        'precipitation': 0.1,
        'demand': 7200
    }
    
    response = requests.post('http://localhost:5000/predict', json=data)
    result = response.json()
    
    print(f"Production pr√©dite: {result['prediction_mw']:.0f} MW")

Quelles sont les limites du mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Limitations techniques** :

1. **Horizon de pr√©diction** : Optimis√© pour 1-7 jours
2. **Donn√©es requises** : Besoin de toutes les variables m√©t√©o
3. **Domaine g√©ographique** : Calibr√© pour les conditions du dataset BanE-16
4. **Saisonnalit√©** : Performance peut varier selon les saisons

**Limitations op√©rationnelles** :

- **Events extr√™mes** : Moins pr√©cis lors de conditions m√©t√©o exceptionnelles
- **Maintenance** : D√©gradation possible sans r√©entra√Ænement r√©gulier
- **Nouveaux patterns** : Adaptation n√©cessaire pour changements technologiques

**Mitigation** :

.. code-block:: python

    # Monitoring de la d√©rive
    def detect_model_drift(new_predictions, historical_residuals):
        current_error = np.std(new_predictions)
        historical_error = np.std(historical_residuals)
        
        drift_ratio = current_error / historical_error
        
        if drift_ratio > 1.5:
            return "WARNING: Model drift detected"
        elif drift_ratio > 1.2:
            return "CAUTION: Monitor model performance"
        else:
            return "OK: Model performance stable"

Questions de maintenance
------------------------

√Ä quelle fr√©quence r√©entra√Æner le mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Recommandations par contexte** :

.. list-table:: Fr√©quence de r√©entra√Ænement
   :widths: 30 25 45
   :header-rows: 1

   * - Contexte
     - Fr√©quence
     - Indicateurs de besoin
   * - Production stable
     - 3-6 mois
     - RMSE augmente de >10%
   * - Environnement changeant
     - 1-2 mois
     - Nouveaux patterns m√©t√©o
   * - Apr√®s maintenance
     - Imm√©diat
     - Changements √©quipements
   * - Donn√©es enrichies
     - D√®s disponibilit√©
     - Nouvelles variables utiles

**Processus automatis√©** :

.. code-block:: python

    def automated_retraining_check():
        # 1. Charger nouvelles donn√©es
        new_data = load_recent_data(days=30)
        
        # 2. √âvaluer performance actuelle
        current_model = load_model('current_model.h5')
        rmse_current = evaluate_model(current_model, new_data)
        
        # 3. Comparer avec baseline
        rmse_baseline = load_baseline_metric()
        
        # 4. D√©cision de r√©entra√Ænement
        if rmse_current > rmse_baseline * 1.15:
            print("üîÑ R√©entra√Ænement recommand√©")
            retrain_model(new_data)
        else:
            print("‚úÖ Mod√®le performant, pas de r√©entra√Ænement")

Comment sauvegarder et restaurer les mod√®les ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Strat√©gie de versioning** :

.. code-block:: python

    import datetime
    import shutil
    
    def save_model_version(model, scaler_X, scaler_y, metrics):
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        version_dir = f"models/v_{timestamp}"
        
        os.makedirs(version_dir, exist_ok=True)
        
        # Sauvegarde mod√®le et scalers
        model.save(f"{version_dir}/model.h5")
        joblib.dump(scaler_X, f"{version_dir}/scaler_X.pkl")
        joblib.dump(scaler_y, f"{version_dir}/scaler_y.pkl")
        
        # M√©tadonn√©es
        metadata = {
            'timestamp': timestamp,
            'rmse': metrics['rmse'],
            'mae': metrics['mae'],
            'mape': metrics['mape'],
            'r2': metrics['r2'],
            'training_data_size': metrics['data_size'],
            'hyperparameters': metrics['hyperparams']
        }
        
        with open(f"{version_dir}/metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"‚úÖ Mod√®le sauvegard√©: {version_dir}")
        
        return version_dir

**Comparaison de versions** :

.. code-block:: python

    def compare_model_versions():
        versions = glob.glob("models/v_*")
        comparison = []
        
        for version in versions:
            with open(f"{version}/metadata.json", 'r') as f:
                metadata = json.load(f)
            comparison.append(metadata)
        
        df = pd.DataFrame(comparison)
        df = df.sort_values('rmse')
        
        print("üèÜ Comparaison des mod√®les:")
        print(df[['timestamp', 'rmse', 'mae', 'r2']].head())
        
        return df.iloc[0]['timestamp']  # Meilleur mod√®le

Support et communaut√©
---------------------

O√π obtenir de l'aide ?
~~~~~~~~~~~~~~~~~~~~~~~

**Ressources officielles** :

1. **Documentation** : Cette documentation compl√®te
2. **Issues GitHub** : Pour bugs et demandes de fonctionnalit√©s
3. **Notebooks d'exemples** : Cas d'usage pratiques

**Communaut√©** :

- Forums de discussion sur l'analyse √©nerg√©tique
- Groupes TensorFlow/Keras pour questions techniques
- Communaut√© Streamlit pour l'interface

Comment contribuer au projet ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Types de contributions** :

1. **Code** : Am√©liorations, nouvelles fonctionnalit√©s
2. **Documentation** : Corrections, exemples suppl√©mentaires
3. **Tests** : Validation sur nouveaux datasets
4. **Feedback** : Retours d'exp√©rience utilisateur

**Processus** :

.. code-block:: bash

    # 1. Fork du repository
    git clone https://github.com/your-username/AnalysingEnergy.git
    
    # 2. Cr√©ation branche
    git checkout -b feature/nouvelle-fonctionnalite
    
    # 3. D√©veloppement et tests
    # ... votre code ...
    
    # 4. Pull request
    git push origin feature/nouvelle-fonctionnalite

Ressources suppl√©mentaires
---------------------------

**Documentation externe** :

- `TensorFlow Guide <https://www.tensorflow.org/guide>`_
- `Streamlit Documentation <https://docs.streamlit.io>`_
- `Optuna Tutorials <https://optuna.readthedocs.io>`_

**Datasets similaires** :

- Open Power System Data
- ENTSO-E Transparency Platform
- Global Energy Observatory

**Articles de recherche** :

- "Deep Learning for Energy Forecasting" (Nature Energy, 2023)
- "LSTM Networks for Renewable Energy Prediction" (IEEE, 2023)
- "Time Series Analysis in Energy Systems" (Journal of Cleaner Production, 2023)
------------------

Qu'est-ce que le projet AnalysingEnergy ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Le projet AnalysingEnergy est un syst√®me de pr√©diction de g√©n√©ration d'√©nergie verte utilisant des r√©seaux de neurones LSTM (Long Short-Term Memory). Il analyse les donn√©es m√©t√©orologiques du dataset BanE-16 pour pr√©dire la production √©nerg√©tique quotidienne avec une pr√©cision √©lev√©e (RMSE: 291.19 MW).

**Caract√©ristiques principales :**

- Mod√®les LSTM optimis√©s avec Optuna
- Interface utilisateur Streamlit intuitive  
- Pr√©dictions court et long terme
- Visualisations interactives
- Documentation compl√®te

Quelle est la pr√©cision du mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Notre mod√®le LSTM optimis√© atteint les performances suivantes :

- **RMSE** : 291.19 MW (8.9% de la capacit√© maximale)
- **MAE** : ~185 MW
- **R¬≤** : 0.847
- **Corr√©lation** : 0.921

Ces m√©triques placent notre mod√®le bien au-dessus des m√©thodes de r√©f√©rence (persistence, moyenne mobile, etc.).

Puis-je utiliser le projet avec mes propres donn√©es ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Oui, le projet est con√ßu pour √™tre adaptable. Vos donn√©es doivent contenir :

**Colonnes requises :**

- Variables m√©t√©orologiques : temp√©rature (min/mean/max), vitesse du vent (min/mean/max), pr√©cipitations, pression, humidit√©
- Variable cible : g√©n√©ration d'√©nergie

**Format :**

.. code-block:: python

   # Exemple de format attendu
   required_columns = [
       'min_temperature', 'mean_temperature', 'max_temperature',
       'min_windspeed', 'mean_windspeed', 'max_windspeed', 
       'total_precipitation', 'surface_pressure', 'mean_relative_humidity',
       'max_generation(mw)'  # Variable cible
   ]

Pour adapter vos donn√©es :

.. code-block:: python

   # Mapper vos colonnes vers le format attendu
   column_mapping = {
       'your_temp_min': 'min_temperature',
       'your_temp_avg': 'mean_temperature',
       'your_generation': 'max_generation(mw)',
       # ... autres mappings
   }
   
   data = data.rename(columns=column_mapping)

Questions techniques
-------------------

Quelles sont les d√©pendances requises ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**D√©pendances principales :**

- Python 3.8+
- TensorFlow 2.13.0
- Pandas 1.5+
- NumPy 1.24+
- Scikit-learn 1.3+
- Streamlit 1.28+
- Optuna 3.4+
- Plotly 5.17+

**Installation compl√®te :**

.. code-block:: bash

   pip install -r requirements.txt

Pour cr√©er le fichier requirements.txt :

.. code-block:: text

   pandas>=1.5.0
   numpy>=1.24.0
   matplotlib>=3.7.0
   seaborn>=0.12.0
   scikit-learn>=1.3.0
   tensorflow>=2.13.0
   streamlit>=1.28.0
   plotly>=5.17.0
   optuna>=3.4.0

Comment optimiser les performances ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Pour l'entra√Ænement :**

1. **Utiliser un GPU** si disponible
   
   .. code-block:: python
   
      import tensorflow as tf
      print(f"GPU disponible: {tf.config.list_physical_devices('GPU')}")

2. **Optimiser les param√®tres**
   
   .. code-block:: python
   
      # Param√®tres optimaux trouv√©s
      optimal_params = {
           'units_1': 74,
           'units_2': 69, 
           'dropout_rate': 0.1938,
           'batch_size': 32,
           'learning_rate': 0.001
       }

3. **Pipeline de donn√©es efficace**
   
   .. code-block:: python
   
      # Utiliser tf.data pour de meilleures performances
      dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
      dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)

**Pour les pr√©dictions :**

- Pr√©dire par batches plut√¥t qu'individuellement
- Utiliser des mod√®les quantifi√©s pour le d√©ploiement
- Mettre en cache les pr√©dictions fr√©quentes

Pourquoi utiliser LSTM plut√¥t que d'autres mod√®les ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Avantages des LSTM pour la pr√©diction √©nerg√©tique :**

1. **M√©moire long terme** : Capture les patterns saisonniers et cycliques
2. **Gestion des s√©quences** : Id√©al pour les s√©ries temporelles
3. **Non-lin√©arit√©** : Mod√©lise les relations complexes m√©t√©o-√©nergie
4. **Robustesse** : G√®re bien les valeurs manquantes et le bruit

**Comparaison avec d'autres approches :**

.. code-block:: text

   Mod√®le              RMSE (MW)    R¬≤      Avantages
   ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
   LSTM (optimis√©)     291.19      0.847   Meilleure pr√©cision
   Random Forest       340.25      0.782   Interpr√©tabilit√©  
   ARIMA              425.67      0.651   Simplicit√©
   R√©gression Lin√©aire 580.12      0.423   Rapidit√©
   Persistence        612.45      0.385   Baseline simple

Questions sur l'utilisation
---------------------------

Comment faire une pr√©diction simple ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Via l'interface Streamlit :**

1. Lancer l'application : `streamlit run interface/app.py`
2. Entrer les param√®tres m√©t√©orologiques
3. Cliquer sur "Pr√©dire"

**Via code Python :**

.. code-block:: python

   from interface.app import EnergyPredictor
   
   # Initialisation
   predictor = EnergyPredictor()
   predictor.load_trained_models()
   
   # Pr√©diction
   prediction = predictor.predict_single_day(
       min_temp=18.0, mean_temp=25.0, max_temp=32.0,
       wind_speed=15.0, precipitation=0.0,
       pressure=1013.0, humidity=60.0
   )
   
   print(f"G√©n√©ration pr√©dite: {prediction:.2f} MW")

Comment interpr√©ter les r√©sultats ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Unit√©s et √©chelles :**

- Pr√©dictions en **m√©gawatts (MW)**
- Valeurs typiques : 0-5000 MW selon la capacit√© install√©e
- Intervalle de confiance √† consid√©rer (¬±291 MW en moyenne)

**Facteurs d'influence :**

1. **Vitesse du vent** : Impact le plus important (corr√©lation ~0.75)
2. **Temp√©rature** : Impact mod√©r√© (corr√©lation ~0.45)  
3. **Saison** : Variations saisonni√®res marqu√©es
4. **Conditions m√©t√©o extr√™mes** : Peuvent causer des √©carts

**Exemple d'interpr√©tation :**

.. code-block:: text

   Pr√©diction: 1250 MW
   
   Interpr√©tation:
   - Production √©lev√©e (> moyenne de 800 MW)
   - Conditions favorables (vent fort, temp√©rature mod√©r√©e)
   - Confiance √©lev√©e (conditions dans la plage d'entra√Ænement)
   - Recommandation: Planifier pour forte production

Comment entra√Æner un nouveau mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Entra√Ænement avec les donn√©es existantes :**

.. code-block:: python

   predictor = EnergyPredictor()
   predictor.load_data('Data/data.csv')
   
   # Entra√Ænement avec param√®tres par d√©faut
   history = predictor.train_models(epochs=100, batch_size=32)
   
   # √âvaluation
   metrics = predictor.evaluate_models()
   print(f"RMSE: {metrics['RMSE']:.2f}")

**Entra√Ænement avec optimisation :**

.. code-block:: python

   # Optimisation des hyperparam√®tres (plus long)
   from your_module import ModelOptimizer
   
   optimizer = ModelOptimizer(X_train, y_train, X_val, y_val)
   best_params = optimizer.optimize(n_trials=50)
   
   # Utilisation des meilleurs param√®tres
   predictor.train_models(**best_params)

**Temps d'entra√Ænement typiques :**

- CPU : 2-4 heures (selon les param√®tres)
- GPU : 30-60 minutes
- Optimisation : 8-12 heures (50 trials)

Questions sur les donn√©es
------------------------

Quelle est la qualit√© requise des donn√©es ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Crit√®res minimaux :**

- **Compl√©tude** : < 10% de valeurs manquantes par variable
- **Coh√©rence** : Valeurs dans des plages r√©alistes
- **Fr√©quence** : Donn√©es quotidiennes (minimum)
- **Dur√©e** : Au moins 2 ans pour capturer la saisonnalit√©

**V√©rification de qualit√© :**

.. code-block:: python

   def check_data_quality(data):
       """V√©rification de la qualit√© des donn√©es"""
       
       report = {}
       
       # Valeurs manquantes
       missing_pct = (data.isnull().sum() / len(data)) * 100
       report['missing_data'] = missing_pct[missing_pct > 0].to_dict()
       
       # Valeurs aberrantes (m√©thode IQR)
       numeric_cols = data.select_dtypes(include=[np.number]).columns
       outliers = {}
       for col in numeric_cols:
           Q1, Q3 = data[col].quantile([0.25, 0.75])
           IQR = Q3 - Q1
           outlier_count = ((data[col] < Q1 - 1.5*IQR) | 
                           (data[col] > Q3 + 1.5*IQR)).sum()
           outliers[col] = outlier_count
       report['outliers'] = outliers
       
       # Plages de valeurs
       report['value_ranges'] = data.describe().to_dict()
       
       return report

Comment g√©rer les valeurs manquantes ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Strat√©gies selon le pourcentage :**

- **< 5%** : Interpolation lin√©aire
- **5-15%** : M√©thodes avanc√©es (KNN, r√©gression)
- **> 15%** : Analyse de la cause, possible exclusion

**Impl√©mentation :**

.. code-block:: python

   # Interpolation simple
   data['temperature'] = data['temperature'].interpolate(method='linear')
   
   # Imputation KNN pour patterns complexes
   from sklearn.impute import KNNImputer
   
   imputer = KNNImputer(n_neighbors=5)
   data_imputed = imputer.fit_transform(data[numeric_columns])

Puis-je utiliser des donn√©es horaires ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Le mod√®le actuel est optimis√© pour des donn√©es quotidiennes. Pour des donn√©es horaires :

**Adaptations n√©cessaires :**

1. **Agr√©gation** vers le quotidien
   
   .. code-block:: python
   
      # Agr√©gation quotidienne
      daily_data = hourly_data.resample('D').agg({
           'temperature': 'mean',
           'wind_speed': 'mean', 
           'generation': 'max',  # Pic de g√©n√©ration
           'precipitation': 'sum'
       })

2. **Mod√®le haute fr√©quence** (d√©veloppement futur)
   
   - S√©quences plus courtes (24h au lieu de 60 jours)
   - Architecture adapt√©e
   - Plus de donn√©es d'entra√Ænement requises

Questions sur le d√©ploiement
---------------------------

Comment d√©ployer l'application en production ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Options de d√©ploiement :**

1. **Serveur local**
   
   .. code-block:: bash
   
      # Production locale
      streamlit run interface/app.py --server.port 8501 --server.address 0.0.0.0

2. **Cloud (Heroku, AWS, etc.)**
   
   Cr√©er un `Procfile` :
   
   .. code-block:: text
   
      web: streamlit run interface/app.py --server.port=$PORT --server.address=0.0.0.0

3. **Conteneurisation Docker**
   
   .. code-block:: dockerfile
   
      FROM python:3.9-slim
      
      WORKDIR /app
      COPY requirements.txt .
      RUN pip install -r requirements.txt
      
      COPY . .
      
      EXPOSE 8501
      CMD ["streamlit", "run", "interface/app.py"]

Comment int√©grer via API REST ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Cr√©ation d'une API Flask :**

.. code-block:: python

   from flask import Flask, request, jsonify
   from interface.app import EnergyPredictor
   
   app = Flask(__name__)
   predictor = EnergyPredictor()
   predictor.load_trained_models()
   
   @app.route('/predict', methods=['POST'])
   def predict():
       data = request.json
       
       prediction = predictor.predict_single_day(
           min_temp=data['min_temp'],
           mean_temp=data['mean_temp'],
           max_temp=data['max_temp'],
           wind_speed=data['wind_speed'],
           precipitation=data['precipitation'],
           pressure=data['pressure'],
           humidity=data['humidity']
       )
       
       return jsonify({'prediction': float(prediction)})
   
   if __name__ == '__main__':
       app.run(debug=False, host='0.0.0.0', port=5000)

**Utilisation de l'API :**

.. code-block:: python

   import requests
   
   # Appel √† l'API
   response = requests.post('http://localhost:5000/predict', json={
       'min_temp': 18.0,
       'mean_temp': 25.0,
       'max_temp': 32.0,
       'wind_speed': 15.0,
       'precipitation': 0.0,
       'pressure': 1013.0,
       'humidity': 60.0
   })
   
   prediction = response.json()['prediction']
   print(f"Pr√©diction: {prediction:.2f} MW")

Quelles sont les limites du mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Limites techniques :**

1. **Horizon de pr√©diction** : Pr√©cision d√©cro√Æt au-del√† de 30 jours
2. **Conditions extr√™mes** : Performance r√©duite lors d'√©v√©nements exceptionnels
3. **G√©n√©ralisation** : Optimis√© pour le dataset BanE-16 sp√©cifique
4. **Variables d'entr√©e** : Limit√© aux variables m√©t√©orologiques disponibles

**Limites pratiques :**

- N√©cessite des pr√©visions m√©t√©orologiques fiables
- Performance d√©pendante de la qualit√© des donn√©es d'entra√Ænement
- R√©entra√Ænement p√©riodique recommand√©

**Am√©liorations futures :**

- Int√©gration de donn√©es satellite
- Mod√®les ensemble pour r√©duire l'incertitude
- Variables √©conomiques et r√©glementaires
- Pr√©dictions probabilistes avec intervalles de confiance

Questions de maintenance
-----------------------

√Ä quelle fr√©quence r√©entra√Æner le mod√®le ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Recommandations :**

- **Mensuellement** : Ajout des nouvelles donn√©es
- **Trimestriellement** : R√©√©valuation compl√®te des performances
- **Annuellement** : Optimisation des hyperparam√®tres

**Indicateurs de d√©gradation :**

.. code-block:: python

   def monitor_model_performance(current_rmse, baseline_rmse=291.19):
       """Surveillance de la performance du mod√®le"""
       
       degradation = (current_rmse - baseline_rmse) / baseline_rmse * 100
       
       if degradation > 20:
           return "R√©entra√Ænement urgent requis"
       elif degradation > 10:
           return "R√©entra√Ænement recommand√©"
       else:
           return "Performance acceptable"

Comment sauvegarder et restaurer les mod√®les ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Sauvegarde compl√®te :**

.. code-block:: python

   import pickle
   from datetime import datetime
   
   # Sauvegarde du mod√®le avec m√©tadonn√©es
   timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
   
   # Mod√®le TensorFlow
   model.save(f'models/lstm_model_{timestamp}.h5')
   
   # Scalers
   with open(f'scalers/scaler_{timestamp}.pkl', 'wb') as f:
       pickle.dump(scaler, f)
   
   # M√©tadonn√©es
   metadata = {
       'rmse': 291.19,
       'training_date': timestamp,
       'hyperparameters': optimal_params,
       'data_version': 'v1.0'
   }
   
   with open(f'models/metadata_{timestamp}.json', 'w') as f:
       json.dump(metadata, f)

**Restauration :**

.. code-block:: python

   # Chargement complet
   model = tf.keras.models.load_model('models/lstm_model_20231201_143022.h5')
   
   with open('scalers/scaler_20231201_143022.pkl', 'rb') as f:
       scaler = pickle.load(f)

Support et communaut√©
--------------------

O√π obtenir de l'aide ?
~~~~~~~~~~~~~~~~~~~~~~

1. **Documentation** : Consultez d'abord cette documentation compl√®te
2. **Troubleshooting** : :doc:`troubleshooting` pour les probl√®mes courants
3. **GitHub Issues** : Pour rapporter des bugs ou demander des fonctionnalit√©s
4. **Email** : Contact direct avec l'√©quipe de d√©veloppement

Comment contribuer au projet ?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Types de contributions :**

- Rapports de bugs
- Suggestions d'am√©liorations
- Nouvelles fonctionnalit√©s
- Documentation
- Tests et validation

**Processus de contribution :**

1. Fork du repository
2. Cr√©ation d'une branche feature
3. D√©veloppement et tests
4. Pull request avec description d√©taill√©e

Ressources suppl√©mentaires
-------------------------

- **Documentation API** : :doc:`api_reference`
- **Notebooks exemples** : :doc:`notebooks/index`
- **Guide d'optimisation** : :doc:`hyperparameter_optimization`
- **Analyse des donn√©es** : :doc:`data_analysis`

.. note::

   Cette FAQ est mise √† jour r√©guli√®rement. N'h√©sitez pas √† sugg√©rer de nouvelles questions qui pourraient aider la communaut√©.
