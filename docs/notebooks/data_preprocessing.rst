Notebook de prÃ©paration des donnÃ©es
===================================

Ce notebook guide Ã  travers le processus complet de prÃ©paration des donnÃ©es pour l'analyse Ã©nergÃ©tique avec les modÃ¨les LSTM.

Vue d'ensemble
--------------

Le notebook ``Data preprocessing.ipynb`` couvre toutes les Ã©tapes nÃ©cessaires pour transformer les donnÃ©es brutes du dataset BanE-16 en donnÃ©es prÃªtes pour l'entraÃ®nement des modÃ¨les LSTM.

Ã‰tapes principales
------------------

1. **Chargement des donnÃ©es**
2. **Analyse exploratoire des donnÃ©es (EDA)**
3. **Nettoyage et filtrage**
4. **Gestion des valeurs manquantes**
5. **DÃ©tection et traitement des outliers**
6. **Normalisation et mise Ã  l'Ã©chelle**
7. **CrÃ©ation des sÃ©quences temporelles**
8. **Division train/validation/test**

Structure du notebook
---------------------

Introduction et imports
~~~~~~~~~~~~~~~~~~~~~~~

Le notebook commence par l'importation des bibliothÃ¨ques nÃ©cessaires et la configuration de l'environnement :

.. code-block:: python

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn.model_selection import train_test_split
    import warnings
    warnings.filterwarnings('ignore')

Chargement et exploration initiale
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Chargement des donnÃ©es brutes :**

.. code-block:: python

    # Chargement du dataset principal
    data = pd.read_csv('Data/data.csv', parse_dates=['date'])
    print(f"Forme du dataset : {data.shape}")
    print(f"PÃ©riode couverte : {data['date'].min()} Ã  {data['date'].max()}")

**Inspection de la structure :**

.. code-block:: python

    # Affichage des informations gÃ©nÃ©rales
    data.info()
    
    # Statistiques descriptives
    data.describe()
    
    # VÃ©rification des valeurs manquantes
    missing_values = data.isnull().sum()
    print("Valeurs manquantes par colonne :")
    print(missing_values[missing_values > 0])

Analyse exploratoire des donnÃ©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Visualisation des tendances temporelles :**

.. code-block:: python

    # Graphique de la production Ã©nergÃ©tique dans le temps
    plt.figure(figsize=(15, 8))
    plt.plot(data['date'], data['max_generation(mw)'], alpha=0.7)
    plt.title('Ã‰volution de la Production Ã‰nergÃ©tique')
    plt.xlabel('Date')
    plt.ylabel('Production (MW)')
    plt.grid(True, alpha=0.3)
    plt.show()

**Analyse des corrÃ©lations :**

.. code-block:: python

    # Matrice de corrÃ©lation
    correlation_matrix = data.select_dtypes(include=[np.number]).corr()
    
    plt.figure(figsize=(12, 10))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
    plt.title('Matrice de CorrÃ©lation des Variables')
    plt.tight_layout()
    plt.show()

Nettoyage des donnÃ©es
~~~~~~~~~~~~~~~~~~~~

**Gestion des valeurs manquantes :**

.. code-block:: python

    # StratÃ©gies de traitement selon le type de variable
    def handle_missing_values(df):
        df_clean = df.copy()
        
        # Variables mÃ©tÃ©orologiques : interpolation linÃ©aire
        weather_cols = ['temp2_max(c)', 'temp2_min(c)', 'temp2_ave(c)',
                       'wind_speed50_max(ms)', 'wind_speed50_min(ms)', 'wind_speed50_ave(ms)',
                       'suface_pressure(pa)', 'prectotcorr']
        
        for col in weather_cols:
            if col in df_clean.columns:
                df_clean[col] = df_clean[col].interpolate(method='linear')
        
        # Variables Ã©nergÃ©tiques : forward fill puis backward fill
        energy_cols = ['max_generation(mw)', 'total_demand(mw)']
        for col in energy_cols:
            if col in df_clean.columns:
                df_clean[col] = df_clean[col].fillna(method='ffill').fillna(method='bfill')
        
        return df_clean

**DÃ©tection des outliers :**

.. code-block:: python

    def detect_outliers(df, columns, threshold=3):
        """DÃ©tecte les outliers en utilisant la mÃ©thode z-score."""
        outliers_dict = {}
        
        for col in columns:
            if col in df.columns:
                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                outliers = df[z_scores > threshold]
                outliers_dict[col] = outliers.index.tolist()
                
                print(f"{col}: {len(outliers)} outliers dÃ©tectÃ©s")
        
        return outliers_dict

**Traitement des outliers :**

.. code-block:: python

    def treat_outliers(df, method='clip', percentile=99):
        """Traite les outliers par clipping ou suppression."""
        df_treated = df.copy()
        
        numeric_cols = df_treated.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if method == 'clip':
                # Clipping aux percentiles 1% et 99%
                lower_bound = df_treated[col].quantile(0.01)
                upper_bound = df_treated[col].quantile(0.99)
                df_treated[col] = df_treated[col].clip(lower_bound, upper_bound)
            
            elif method == 'remove':
                # Suppression des outliers
                Q1 = df_treated[col].quantile(0.25)
                Q3 = df_treated[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                df_treated = df_treated[
                    (df_treated[col] >= lower_bound) & 
                    (df_treated[col] <= upper_bound)
                ]
        
        return df_treated

Normalisation et mise Ã  l'Ã©chelle
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**PrÃ©paration des scalers :**

.. code-block:: python

    # SÃ©paration des features et target
    feature_columns = ['temp2_max(c)', 'temp2_min(c)', 'temp2_ave(c)',
                      'suface_pressure(pa)', 'wind_speed50_max(ms)', 
                      'wind_speed50_min(ms)', 'wind_speed50_ave(ms)',
                      'prectotcorr', 'total_demand(mw)']
    
    target_column = 'max_generation(mw)'
    
    X = data_clean[feature_columns].values
    y = data_clean[target_column].values.reshape(-1, 1)

**Application des scalers :**

.. code-block:: python

    # Normalisation des features
    scaler_X = StandardScaler()
    X_scaled = scaler_X.fit_transform(X)
    
    # Normalisation du target
    scaler_y = StandardScaler()
    y_scaled = scaler_y.fit_transform(y)
    
    print(f"Forme des features normalisÃ©es : {X_scaled.shape}")
    print(f"Forme du target normalisÃ© : {y_scaled.shape}")

CrÃ©ation des sÃ©quences temporelles
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Fonction de crÃ©ation de sÃ©quences :**

.. code-block:: python

    def create_sequences(X, y, time_steps=60):
        """CrÃ©e des sÃ©quences temporelles pour l'entraÃ®nement LSTM."""
        X_seq, y_seq = [], []
        
        for i in range(len(X) - time_steps):
            X_seq.append(X[i:(i + time_steps)])
            y_seq.append(y[i + time_steps])
        
        return np.array(X_seq), np.array(y_seq)

**Application :**

.. code-block:: python

    # ParamÃ¨tres de sÃ©quence
    TIME_STEPS = 60  # 60 jours d'historique
    
    # CrÃ©ation des sÃ©quences
    X_sequences, y_sequences = create_sequences(X_scaled, y_scaled, TIME_STEPS)
    
    print(f"Forme des sÃ©quences X : {X_sequences.shape}")
    print(f"Forme des sÃ©quences y : {y_sequences.shape}")

Division des donnÃ©es
~~~~~~~~~~~~~~~~~~~

**Split train/validation/test :**

.. code-block:: python

    # Division temporelle pour respecter l'ordre chronologique
    train_size = int(0.7 * len(X_sequences))
    val_size = int(0.2 * len(X_sequences))
    
    X_train = X_sequences[:train_size]
    y_train = y_sequences[:train_size]
    
    X_val = X_sequences[train_size:train_size + val_size]
    y_val = y_sequences[train_size:train_size + val_size]
    
    X_test = X_sequences[train_size + val_size:]
    y_test = y_sequences[train_size + val_size:]
    
    print(f"Train: {X_train.shape[0]} Ã©chantillons")
    print(f"Validation: {X_val.shape[0]} Ã©chantillons")
    print(f"Test: {X_test.shape[0]} Ã©chantillons")

Sauvegarde des donnÃ©es prÃ©parÃ©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Export des donnÃ©es traitÃ©es :**

.. code-block:: python

    # Sauvegarde des donnÃ©es
    np.save('Data/X_train.npy', X_train)
    np.save('Data/y_train.npy', y_train)
    np.save('Data/X_val.npy', X_val)
    np.save('Data/y_val.npy', y_val)
    np.save('Data/X_test.npy', X_test)
    np.save('Data/y_test.npy', y_test)

**Sauvegarde des scalers :**

.. code-block:: python

    import joblib
    
    # Sauvegarde des scalers pour utilisation future
    joblib.dump(scaler_X, 'Notebooks/scalers/X_train_scaler.pkl')
    joblib.dump(scaler_y, 'Notebooks/scalers/y_train_scaler.pkl')

Validation de la prÃ©paration
~~~~~~~~~~~~~~~~~~~~~~~~~~~

**VÃ©rifications finales :**

.. code-block:: python

    # VÃ©rification de la qualitÃ© des donnÃ©es
    def validate_preprocessing(X_train, y_train, X_val, y_val, X_test, y_test):
        """Valide la qualitÃ© de la prÃ©paration des donnÃ©es."""
        
        # VÃ©rification des formes
        assert X_train.shape[1:] == X_val.shape[1:] == X_test.shape[1:]
        assert y_train.shape[1:] == y_val.shape[1:] == y_test.shape[1:]
        
        # VÃ©rification de l'absence de NaN
        assert not np.isnan(X_train).any()
        assert not np.isnan(y_train).any()
        assert not np.isnan(X_val).any()
        assert not np.isnan(y_val).any()
        assert not np.isnan(X_test).any()
        assert not np.isnan(y_test).any()
        
        # VÃ©rification des distributions
        print("Statistiques des donnÃ©es d'entraÃ®nement :")
        print(f"X_train - Min: {X_train.min():.3f}, Max: {X_train.max():.3f}")
        print(f"y_train - Min: {y_train.min():.3f}, Max: {y_train.max():.3f}")
        
        print("âœ… Validation rÃ©ussie !")
    
    validate_preprocessing(X_train, y_train, X_val, y_val, X_test, y_test)

RÃ©sumÃ© des transformations
~~~~~~~~~~~~~~~~~~~~~~~~~

**RÃ©capitulatif des Ã©tapes appliquÃ©es :**

.. code-block:: python

    # Affichage du rÃ©sumÃ© de prÃ©paration
    summary = {
        'Dataset original': data.shape,
        'AprÃ¨s nettoyage': data_clean.shape,
        'Features sÃ©lectionnÃ©es': len(feature_columns),
        'SÃ©quences crÃ©Ã©es': X_sequences.shape[0],
        'Longueur sÃ©quence': TIME_STEPS,
        'Train/Val/Test': f"{len(X_train)}/{len(X_val)}/{len(X_test)}"
    }
    
    print("ðŸ“Š RÃ©sumÃ© de la prÃ©paration des donnÃ©es :")
    for key, value in summary.items():
        print(f"  {key}: {value}")

Utilisation pratique
-------------------

**ExÃ©cution du notebook :**

1. Assurez-vous que le fichier ``Data/data.csv`` est prÃ©sent
2. ExÃ©cutez toutes les cellules dans l'ordre
3. VÃ©rifiez la crÃ©ation des fichiers de sortie dans ``Data/`` et ``Notebooks/scalers/``

**Personnalisation :**

- Modifiez ``TIME_STEPS`` pour changer la longueur des sÃ©quences
- Ajustez les mÃ©thodes de traitement des outliers selon vos besoins
- Adaptez les ratios de division train/val/test

**Fichiers gÃ©nÃ©rÃ©s :**

- ``X_train.npy``, ``y_train.npy`` : DonnÃ©es d'entraÃ®nement
- ``X_val.npy``, ``y_val.npy`` : DonnÃ©es de validation
- ``X_test.npy``, ``y_test.npy`` : DonnÃ©es de test
- ``X_train_scaler.pkl``, ``y_train_scaler.pkl`` : Scalers sauvegardÃ©s

Prochaines Ã©tapes
----------------

AprÃ¨s avoir prÃ©parÃ© les donnÃ©es avec ce notebook, vous pouvez :

1. Consulter :doc:`lstm_training` pour l'entraÃ®nement des modÃ¨les
2. Explorer :doc:`../model_evaluation` pour Ã©valuer les performances
3. Utiliser :doc:`prediction` pour faire des prÃ©dictions

Troubleshooting
---------------

**ProblÃ¨mes courants :**

- **Erreur de mÃ©moire** : RÃ©duisez ``TIME_STEPS`` ou le batch size
- **Valeurs manquantes** : VÃ©rifiez la qualitÃ© du dataset source
- **Performance lente** : Utilisez un sous-Ã©chantillon pour les tests

Pour plus d'aide, consultez la section :doc:`../troubleshooting`.
