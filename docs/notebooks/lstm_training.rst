Notebook d'entra√Ænement LSTM
===========================

Ce notebook d√©taille le processus complet d'entra√Ænement des mod√®les LSTM pour la pr√©diction de production d'√©nergie verte.

Vue d'ensemble
--------------

Le notebook ``LSTM complet.ipynb`` couvre l'architecture, l'entra√Ænement et l'optimisation des mod√®les LSTM pour pr√©dire la production √©nerg√©tique bas√©e sur les donn√©es m√©t√©orologiques et de demande.

Objectifs du notebook
---------------------

1. **Conception de l'architecture LSTM**
2. **Configuration des hyperparam√®tres**
3. **Entra√Ænement avec validation crois√©e**
4. **Monitoring des performances**
5. **Sauvegarde des mod√®les optimaux**
6. **√âvaluation et analyse des r√©sultats**

Structure du notebook
---------------------

Imports et configuration
~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
    from tensorflow.keras.regularizers import l1_l2
    
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
    import joblib
    import warnings
    warnings.filterwarnings('ignore')

Chargement des donn√©es pr√©par√©es
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    # Chargement des donn√©es pr√©trait√©es
    X_train = np.load('Data/X_train.npy')
    y_train = np.load('Data/y_train.npy')
    X_val = np.load('Data/X_val.npy')
    y_val = np.load('Data/y_val.npy')
    X_test = np.load('Data/X_test.npy')
    y_test = np.load('Data/y_test.npy')
    
    print(f"Donn√©es d'entra√Ænement: {X_train.shape}")
    print(f"Donn√©es de validation: {X_val.shape}")
    print(f"Donn√©es de test: {X_test.shape}")

Architecture du mod√®le LSTM
~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Mod√®le de base :**

.. code-block:: python

    def create_lstm_model(input_shape, lstm_units=[50, 50], dropout_rate=0.2, 
                         learning_rate=0.001, l1_reg=0.01, l2_reg=0.01):
        """Cr√©e un mod√®le LSTM multi-couches avec r√©gularisation."""
        
        model = Sequential()
        
        # Premi√®re couche LSTM
        model.add(LSTM(
            units=lstm_units[0],
            return_sequences=True,
            input_shape=input_shape,
            kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)
        ))
        model.add(BatchNormalization())
        model.add(Dropout(dropout_rate))
        
        # Couches LSTM additionnelles
        for i, units in enumerate(lstm_units[1:]):
            return_seq = i < len(lstm_units) - 2
            model.add(LSTM(
                units=units,
                return_sequences=return_seq,
                kernel_regularizer=l1_l2(l1=l1_reg, l2=l2_reg)
            ))
            model.add(BatchNormalization())
            model.add(Dropout(dropout_rate))
        
        # Couches denses finales
        model.add(Dense(25, activation='relu'))
        model.add(Dropout(dropout_rate/2))
        model.add(Dense(1, activation='linear'))
        
        # Compilation
        optimizer = Adam(learning_rate=learning_rate)
        model.compile(
            optimizer=optimizer,
            loss='mean_squared_error',
            metrics=['mae']
        )
        
        return model

**Architecture avanc√©e :**

.. code-block:: python

    def create_advanced_lstm_model(input_shape):
        """Mod√®le LSTM avanc√© avec architecture optimis√©e."""
        
        model = Sequential([
            # Premi√®re couche LSTM bidirectionnelle
            tf.keras.layers.Bidirectional(LSTM(
                64, return_sequences=True, 
                kernel_regularizer=l1_l2(l1=0.01, l2=0.01)
            ), input_shape=input_shape),
            BatchNormalization(),
            Dropout(0.3),
            
            # Deuxi√®me couche LSTM
            LSTM(32, return_sequences=True,
                 kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
            BatchNormalization(),
            Dropout(0.2),
            
            # Troisi√®me couche LSTM
            LSTM(16, return_sequences=False,
                 kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),
            BatchNormalization(),
            Dropout(0.2),
            
            # Couches denses avec connexions r√©siduelles
            Dense(32, activation='relu'),
            Dropout(0.1),
            Dense(16, activation='relu'),
            Dense(1, activation='linear')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='huber',  # Plus robuste aux outliers
            metrics=['mae', 'mse']
        )
        
        return model

Configuration des callbacks
~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    def setup_callbacks(model_name='lstm_model'):
        """Configure les callbacks pour l'entra√Ænement."""
        
        callbacks = [
            # Arr√™t pr√©coce
            EarlyStopping(
                monitor='val_loss',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            
            # Sauvegarde du meilleur mod√®le
            ModelCheckpoint(
                filepath=f'Notebooks/models/{model_name}_best.h5',
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=False,
                verbose=1
            ),
            
            # R√©duction du learning rate
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=8,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        return callbacks

Entra√Ænement du mod√®le
~~~~~~~~~~~~~~~~~~~~~~

**Entra√Ænement principal :**

.. code-block:: python

    # Cr√©ation du mod√®le
    input_shape = (X_train.shape[1], X_train.shape[2])
    model = create_lstm_model(input_shape)
    
    # Affichage de l'architecture
    model.summary()
    
    # Configuration des callbacks
    callbacks = setup_callbacks('final_model')
    
    # Entra√Ænement
    history = model.fit(
        X_train, y_train,
        epochs=100,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1,
        shuffle=False  # Important pour les s√©ries temporelles
    )

**Entra√Ænement avec validation crois√©e temporelle :**

.. code-block:: python

    def time_series_cv_training(X, y, n_splits=5):
        """Entra√Ænement avec validation crois√©e temporelle."""
        
        cv_scores = []
        models = []
        
        # Division temporelle
        total_samples = len(X)
        split_size = total_samples // n_splits
        
        for i in range(n_splits):
            print(f"\n--- Fold {i+1}/{n_splits} ---")
            
            # D√©finition des indices
            train_end = (i + 1) * split_size
            val_start = train_end
            val_end = min(val_start + split_size // 2, total_samples)
            
            # Division des donn√©es
            X_fold_train = X[:train_end]
            y_fold_train = y[:train_end]
            X_fold_val = X[val_start:val_end]
            y_fold_val = y[val_start:val_end]
            
            # Cr√©ation et entra√Ænement du mod√®le
            model = create_lstm_model(input_shape)
            callbacks = setup_callbacks(f'model_fold_{i+1}')
            
            history = model.fit(
                X_fold_train, y_fold_train,
                epochs=50,
                batch_size=32,
                validation_data=(X_fold_val, y_fold_val),
                callbacks=callbacks,
                verbose=0
            )
            
            # √âvaluation
            val_loss = min(history.history['val_loss'])
            cv_scores.append(val_loss)
            models.append(model)
            
            print(f"Validation Loss: {val_loss:.4f}")
        
        return models, cv_scores

Monitoring et visualisation
~~~~~~~~~~~~~~~~~~~~~~~~~~

**Visualisation de l'entra√Ænement :**

.. code-block:: python

    def plot_training_history(history):
        """Visualise l'historique d'entra√Ænement."""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss
        axes[0, 0].plot(history.history['loss'], label='Train Loss')
        axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')
        axes[0, 0].set_title('√âvolution de la Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # MAE
        axes[0, 1].plot(history.history['mae'], label='Train MAE')
        axes[0, 1].plot(history.history['val_mae'], label='Validation MAE')
        axes[0, 1].set_title('√âvolution de la MAE')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('MAE')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # Learning Rate (si disponible)
        if 'lr' in history.history:
            axes[1, 0].plot(history.history['lr'])
            axes[1, 0].set_title('√âvolution du Learning Rate')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Learning Rate')
            axes[1, 0].set_yscale('log')
            axes[1, 0].grid(True)
        
        # Gradient norms (si disponible)
        axes[1, 1].text(0.5, 0.5, 'M√©triques additionnelles\n(gradient norms, etc.)',
                        ha='center', va='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('M√©triques Avanc√©es')
        
        plt.tight_layout()
        plt.show()

**Monitoring en temps r√©el :**

.. code-block:: python

    class TrainingMonitor(tf.keras.callbacks.Callback):
        """Callback personnalis√© pour le monitoring."""
        
        def __init__(self):
            self.losses = []
            self.val_losses = []
            self.best_val_loss = float('inf')
        
        def on_epoch_end(self, epoch, logs=None):
            self.losses.append(logs['loss'])
            self.val_losses.append(logs['val_loss'])
            
            if logs['val_loss'] < self.best_val_loss:
                self.best_val_loss = logs['val_loss']
                print(f"\nüéØ Nouveau meilleur mod√®le √† l'epoch {epoch+1}: "
                      f"val_loss = {logs['val_loss']:.4f}")

√âvaluation des performances
~~~~~~~~~~~~~~~~~~~~~~~~~~

**M√©triques d'√©valuation :**

.. code-block:: python

    def evaluate_model(model, X_test, y_test, scaler_y):
        """√âvalue le mod√®le sur les donn√©es de test."""
        
        # Pr√©dictions
        y_pred_scaled = model.predict(X_test)
        
        # D√©normalisation
        y_test_original = scaler_y.inverse_transform(y_test)
        y_pred_original = scaler_y.inverse_transform(y_pred_scaled)
        
        # Calcul des m√©triques
        mse = mean_squared_error(y_test_original, y_pred_original)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test_original, y_pred_original)
        r2 = r2_score(y_test_original, y_pred_original)
        
        # M√©triques relatives
        mape = np.mean(np.abs((y_test_original - y_pred_original) / y_test_original)) * 100
        
        print("üìä M√©triques de performance:")
        print(f"  RMSE: {rmse:.2f} MW")
        print(f"  MAE:  {mae:.2f} MW")
        print(f"  R¬≤:   {r2:.4f}")
        print(f"  MAPE: {mape:.2f}%")
        
        return {
            'rmse': rmse, 'mae': mae, 'r2': r2, 'mape': mape,
            'y_true': y_test_original, 'y_pred': y_pred_original
        }

**Analyse des r√©sidus :**

.. code-block:: python

    def analyze_residuals(y_true, y_pred):
        """Analyse des r√©sidus de pr√©diction."""
        
        residuals = y_true.flatten() - y_pred.flatten()
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Distribution des r√©sidus
        axes[0, 0].hist(residuals, bins=50, alpha=0.7, edgecolor='black')
        axes[0, 0].set_title('Distribution des R√©sidus')
        axes[0, 0].set_xlabel('R√©sidus (MW)')
        axes[0, 0].set_ylabel('Fr√©quence')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Q-Q plot
        from scipy import stats
        stats.probplot(residuals, dist="norm", plot=axes[0, 1])
        axes[0, 1].set_title('Q-Q Plot des R√©sidus')
        axes[0, 1].grid(True, alpha=0.3)
        
        # R√©sidus vs pr√©dictions
        axes[1, 0].scatter(y_pred.flatten(), residuals, alpha=0.6)
        axes[1, 0].axhline(y=0, color='red', linestyle='--')
        axes[1, 0].set_title('R√©sidus vs Pr√©dictions')
        axes[1, 0].set_xlabel('Pr√©dictions (MW)')
        axes[1, 0].set_ylabel('R√©sidus (MW)')
        axes[1, 0].grid(True, alpha=0.3)
        
        # R√©sidus dans le temps
        axes[1, 1].plot(residuals, alpha=0.7)
        axes[1, 1].axhline(y=0, color='red', linestyle='--')
        axes[1, 1].set_title('R√©sidus dans le Temps')
        axes[1, 1].set_xlabel('Index Temporel')
        axes[1, 1].set_ylabel('R√©sidus (MW)')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

Optimisation des hyperparam√®tres
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Grid Search pour LSTM :**

.. code-block:: python

    def lstm_grid_search():
        """Recherche par grille des meilleurs hyperparam√®tres."""
        
        param_grid = {
            'lstm_units': [[32, 16], [50, 25], [64, 32], [128, 64]],
            'dropout_rate': [0.1, 0.2, 0.3],
            'learning_rate': [0.001, 0.0005, 0.0001],
            'batch_size': [16, 32, 64]
        }
        
        best_score = float('inf')
        best_params = None
        results = []
        
        import itertools
        
        # Toutes les combinaisons possibles
        keys = param_grid.keys()
        values = param_grid.values()
        
        for combination in itertools.product(*values):
            params = dict(zip(keys, combination))
            
            print(f"Testing: {params}")
            
            # Cr√©ation et entra√Ænement du mod√®le
            model = create_lstm_model(
                input_shape,
                lstm_units=params['lstm_units'],
                dropout_rate=params['dropout_rate'],
                learning_rate=params['learning_rate']
            )
            
            history = model.fit(
                X_train, y_train,
                epochs=30,  # R√©duction pour le grid search
                batch_size=params['batch_size'],
                validation_data=(X_val, y_val),
                verbose=0
            )
            
            # √âvaluation
            val_loss = min(history.history['val_loss'])
            results.append({**params, 'val_loss': val_loss})
            
            if val_loss < best_score:
                best_score = val_loss
                best_params = params
                print(f"  Nouveau meilleur: {val_loss:.4f}")
        
        return best_params, results

Sauvegarde et export
~~~~~~~~~~~~~~~~~~~

**Sauvegarde du mod√®le final :**

.. code-block:: python

    # Sauvegarde du mod√®le complet
    final_model_path = 'Notebooks/models/final_model.h5'
    model.save(final_model_path)
    
    # Sauvegarde avec m√©tadonn√©es
    model_info = {
        'architecture': 'LSTM multi-couches',
        'input_shape': input_shape,
        'final_val_loss': min(history.history['val_loss']),
        'training_epochs': len(history.history['loss']),
        'hyperparameters': {
            'lstm_units': [50, 25],
            'dropout_rate': 0.2,
            'learning_rate': 0.001
        }
    }
    
    import json
    with open('Notebooks/models/model_info.json', 'w') as f:
        json.dump(model_info, f, indent=2)

**Export des poids et architecture :**

.. code-block:: python

    # Sauvegarde de l'architecture seule
    model_json = model.to_json()
    with open('Notebooks/models/model_architecture.json', 'w') as f:
        f.write(model_json)
    
    # Sauvegarde des poids seuls
    model.save_weights('Notebooks/models/model_weights.h5')

Analyse comparative des mod√®les
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Comparaison de diff√©rentes architectures :**

.. code-block:: python

    def compare_model_architectures():
        """Compare diff√©rentes architectures LSTM."""
        
        architectures = {
            'Simple LSTM': lambda: create_lstm_model(input_shape, [32]),
            'Deep LSTM': lambda: create_lstm_model(input_shape, [64, 32, 16]),
            'Wide LSTM': lambda: create_lstm_model(input_shape, [128, 64]),
            'Advanced LSTM': lambda: create_advanced_lstm_model(input_shape)
        }
        
        results = {}
        
        for name, model_func in architectures.items():
            print(f"\nüîß Entra√Ænement: {name}")
            
            model = model_func()
            callbacks = setup_callbacks(f'comparison_{name.lower().replace(" ", "_")}')
            
            history = model.fit(
                X_train, y_train,
                epochs=50,
                batch_size=32,
                validation_data=(X_val, y_val),
                callbacks=callbacks,
                verbose=0
            )
            
            # √âvaluation
            scaler_y = joblib.load('Notebooks/scalers/y_train_scaler.pkl')
            metrics = evaluate_model(model, X_test, y_test, scaler_y)
            
            results[name] = {
                'val_loss': min(history.history['val_loss']),
                'test_rmse': metrics['rmse'],
                'test_r2': metrics['r2'],
                'parameters': model.count_params()
            }
            
            print(f"  Val Loss: {results[name]['val_loss']:.4f}")
            print(f"  Test RMSE: {results[name]['test_rmse']:.2f}")
            print(f"  Parameters: {results[name]['parameters']:,}")
        
        return results

Utilisation pratique
-------------------

**Ex√©cution du notebook :**

1. Assurez-vous que les donn√©es pr√©trait√©es sont disponibles
2. Ex√©cutez les cellules d'architecture et configuration
3. Lancez l'entra√Ænement avec monitoring
4. √âvaluez les performances sur les donn√©es de test

**Personnalisation :**

- Modifiez l'architecture selon vos besoins de complexit√©
- Ajustez les hyperparam√®tres via grid search
- Adaptez les callbacks selon votre strat√©gie d'entra√Ænement

**Optimisation des performances :**

- Utilisez GPU si disponible : ``tf.config.experimental.set_visible_devices``
- Optimisez les batch sizes selon votre m√©moire
- Impl√©mentez l'entra√Ænement distribu√© pour les gros datasets

Prochaines √©tapes
----------------

Apr√®s l'entra√Ænement avec ce notebook :

1. Consultez :doc:`prediction` pour utiliser le mod√®le entra√Æn√©
2. Explorez :doc:`../model_evaluation` pour une √©valuation approfondie
3. Visitez :doc:`../hyperparameter_optimization` pour l'optimisation avanc√©e

Troubleshooting
---------------

**Probl√®mes d'entra√Ænement :**

- **Overfitting** : Augmentez le dropout, ajoutez de la r√©gularisation
- **Underfitting** : Augmentez la complexit√© du mod√®le, r√©duisez la r√©gularisation
- **Convergence lente** : Ajustez le learning rate, v√©rifiez les donn√©es

Pour plus d'aide, consultez :doc:`../troubleshooting`.
